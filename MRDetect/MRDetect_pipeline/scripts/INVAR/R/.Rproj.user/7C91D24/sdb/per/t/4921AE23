{
    "collab_server" : "",
    "contents" : "# Parse ----------------------------\nget.error_rate_polishing_comparison <- function(error_file, SLX_layout, exclude_PPC = T){\n  # load background rates\n  background_error <- calculate.background_error(error_file, SLX_layout, exclude_PPC = T)\n  background_error <- add.missing_error_classes(background_error, trinucleotide_depth)\n  background_error <- combine_classes(background_error)\n  \n  # set levels\n  background_error$data <- factor(background_error$data, levels = c(\"one_read\", \"both_reads\", \"locus_noise\", \"locus_noise.both_reads\"))\n  \n  print(\"plotting individual error rates\")\n  filter(background_error, case_or_control == \"case\", data == \"locus_noise.both_reads\") %>%\n    ggplot(aes(x = reorder(TRINUCLEOTIDE, background_AF),  y = background_AF, colour = mut_class))+\n    geom_point()+\n    theme_bw()+\n    theme(axis.text.x = element_text(angle = 90, hjust = 0.5),\n          axis.text=element_text(size=10),\n          axis.title=element_text(size=14,face=\"bold\"))+\n    scale_y_log10(breaks = c(1e-7, 1e-6, 1e-5, 1e-4))+\n    labs(x = \"Trinucleotide context\",\n         y = \"Background error rate\",\n         title = paste(\"Background error rates for\", study, setting),\n         subtitle = \"Using cases only\") +\n    guides(fill=FALSE)\n    #facet_grid(mut_class~ ., scales = \"free_x\")\n  \n  ggsave(paste0(plot_dir, \"p7_\", study, \"_\", setting, \".pdf\"), width = 6, height = 4)\n  \n  print(\"Looking into polishing strategies\")\n  # non changing variables\n  error_polishing_settings <- unique(background_error$data)\n  one_read_error_rates <- filter(background_error, data == \"one_read\")\n  names(one_read_error_rates)[names(one_read_error_rates) == \"background_AF\"] <- \"background_AF.one_read\"\n  \n  # looping through the settings\n  for (i in 1:length(error_polishing_settings)){\n    print(error_polishing_settings[i])\n    \n    two_read_error_rates <- filter(background_error, data == error_polishing_settings[i])\n    names(two_read_error_rates)[names(two_read_error_rates) == \"background_AF\"] <- \"other_setting\"\n    \n    error_rates_comparison <- inner_join(one_read_error_rates, two_read_error_rates[,c(\"TRINUCLEOTIDE\", \"case_or_control\", \"ALT\", \"other_setting\")], by = c(\"TRINUCLEOTIDE\", \"ALT\", \"case_or_control\"))\n    \n    if (error_polishing_settings[i] == \"locus_noise.both_reads\"){\n      print(\"saving locus_noise.both_reads\")\n      error_rates_comparison.locus_noise.both_reads <- error_rates_comparison\n    }\n    \n    error_rates_comparison$ratio <- error_rates_comparison$other_setting / error_rates_comparison$background_AF.one_read\n    \n    curr_plot <- ggplot(filter(error_rates_comparison, case_or_control == \"case\"), aes(x = background_AF.one_read, y = other_setting, color = mut_class))+\n      geom_point() +\n      scale_y_log10(limits = c(0.5e-7, 1e-2))+\n      scale_x_log10(limits = c(0.5e-7, 1e-2))+\n      geom_abline(slope = 1, linetype = \"dashed\")+\n      labs(x = \"Error rate pre-filtering\",\n           y = paste(\"Error rate -\", error_polishing_settings[i]),\n           title = error_polishing_settings[i],\n           subtitle = paste(study, setting, \"\\nusing only cases\")) +\n      theme_classic()+\n      guides(fill=FALSE) +\n      theme(axis.text=element_text(size=12),\n            axis.title=element_text(size=14,face=\"bold\"),\n            panel.grid.major = element_line(colour = alpha(\"black\", 0.1)))\n    \n    print(curr_plot)\n    \n    assign(paste0(\"error_polishing_\", error_polishing_settings[i]),curr_plot)\n  }\n  \n  print(\"Now saving all the plots combined\")\n  \n  ggsave(paste0(plot_dir, \"p9_\", study, \"_\", setting, \".pdf\"),\n         plot = multiplot(error_polishing_one_read,\n                          error_polishing_locus_noise,\n                          error_polishing_both_reads,\n                          #error_polishing_polished_only,\n                          error_polishing_locus_noise.both_reads,\n                          cols = 2),\n         width = 12, height = 9)\n  \n  \n  print(\"Making plot to compare filtering steps in a boxplot - p20\")\n  #background_error$data <- factor(background_error$data, levels = c(\"one_read\", \"both_reads\", \"polished_only\", \"polished.both_reads\"))\n  filter(background_error, case_or_control == \"case\")%>%\n  ggplot(aes(x = mut_class, y = background_AF, fill = data)) +\n    geom_boxplot() +\n    scale_y_log10(breaks = c(1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2))+\n    theme_classic() +\n    scale_fill_discrete(name = \"Background filtering\")+\n    labs(x= \"Mutation class\",\n         y= \"Background AF\",\n         title= paste(\"Background error filtering steps\\n\", study, setting))\n  ggsave(paste0(plot_dir, \"p20_\", study, \"_\", setting, \".pdf\"), width = 6, height = 4)\n  \n\n  return(background_error)\n}\n\nget.sample_dates <- function(file){\n  raw <- read.csv(file , skip = 6, header = T)\n  \n  patients <- c(\"01-002\",\"01-003\", \"01-004\", \"01-006\", \"01-010\", \"01-012\", \"01-014\", \"01-020\", \"01-022\", \"01-032\") \n  \n  raw <- raw[raw$PATIENT.STUDY.ID %in% patients,]\n  plasma <- raw[grepl(raw$SAMPLE.TYPE......TUBE.NUMBER, pattern = 'PLAS', ignore.case = T),] \n  \n  plasma$unique <- paste(plasma$PATIENT.STUDY.ID, plasma$PATIENT.INITIALS, plasma$COLLECTION.DATE, sep=\"_\") \n  plasma <- plasma[!duplicated(plasma$unique),c(2,6,53)]\n  \n  plasma$COLLECTION.DATE<- as.Date(plasma$COLLECTION.DATE, \"%d/%m/%y\")\n  \n  plasma <- plyr::ddply(plasma, 'PATIENT.STUDY.ID', function(x){\n    data.frame(x[order(x$COLLECTION.DATE),]) \n  })\n  \n  plasma.timepoints <- data.frame()\n  \n  for (x in 1:length(unique(plasma$PATIENT.STUDY.ID))){\n    temp <- filter(plasma, PATIENT.STUDY.ID == unique(plasma$PATIENT.STUDY.ID)[x])\n    temp$Timepoint <- paste0(\"T\",order(temp$COLLECTION.DATE))\n    plasma.timepoints <- rbind(plasma.timepoints, temp)\n  }\n  \n  plasma.timepoints$COLLECTION.DATE<- lapply(strsplit(plasma.timepoints$unique, split = \"_\"), \"[[\",3)\n  plasma.timepoints <- plasma.timepoints[,c(1,4,2)]\n  return(plasma.timepoints)\n}\n\nmake_sample_dates_file <- function(){\n  sample_dates <- get.sample_dates(file = paste0(tapas_mount_point, \"group_folders/TAPAS/input/MELR/MELR_BIOREPOSITORY_SAMPLELIST_20161005.csv\"))\n  baseline_dates <- read.csv(paste0(tapas_mount_point, \"group_folders/TAPAS/input/MELR/JXP0149_baseline_ctDNA_and_death.csv\"))\n  \n  sample_dates$patient_id <- paste0(\"MR1\", gsub(sample_dates$PATIENT.STUDY.ID, pattern = \".*-(.*)\", replacement = \"\\\\1\"))\n  baseline_dates <- baseline_dates[!is.na(baseline_dates$Start.ctDNA),]\n  \n  baseline_dates$patient_id <- paste0(\"MR1\", gsub(baseline_dates$pt.id, pattern = \"(.*)_.*\", replacement = \"\\\\1\"))\n  \n  sample_dates <- left_join(sample_dates[,2:4], baseline_dates[,c(4,15)], by = \"patient_id\")\n  sample_dates$COLLECTION.DATE <- as.Date(unlist(sample_dates$COLLECTION.DATE), format = \"%d/%m/%y\")\n  sample_dates$Baseline.date <- as.Date(unlist(sample_dates$Baseline.date), format = \"%d/%m/%y\")\n  \n  sample_dates$time <- sample_dates$COLLECTION.DATE - sample_dates$Baseline.date\n  write.csv(sample_dates, \n            file = paste0(tapas_mount_point, \"group_folders/TAPAS/input/MELR/INVAR_MELR_sample_dates.csv\"), \n            quote = F, row.names = F)\n}\ncheck_sample_presence <- function(file){\n  SLX_layout <- read.csv(file = \"~/tapas_pipeline/inputs/combined.SLX_table.180124.csv\",\n                         header = TRUE)\n  SLX_layout$barcode. <- gsub(SLX_layout$barcode., pattern = \"-\", replacement = \"_\")\n  SLX_layout$SLX_barcode <- paste0(SLX_layout$SLX_ID, \"_\", SLX_layout$barcode.)\n  \n  files <- read.table(file)\n  SLX_layout.present  <<- SLX_layout[SLX_layout$SLX_barcode %in% files[,1], ]\n  SLX_layout.missing <<- SLX_layout[!(SLX_layout$SLX_barcode %in% files[,1]),]\n\n}\n\n# preparing code to plot multiple plots in one plot:\nmultiplot <- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {\n  require(grid)\n  \n  plots <- c(list(...), plotlist)\n  \n  numPlots = length(plots)\n  \n  if (is.null(layout)) {\n    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),\n                     ncol = cols, nrow = ceiling(numPlots/cols))\n  }\n  \n  if (numPlots == 1) {\n    print(plots[[1]])\n    \n  } else {\n    grid.newpage()\n    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))\n    \n    for (i in 1:numPlots) {\n      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))\n      \n      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,\n                                      layout.pos.col = matchidx$col))\n    }\n  }\n}\n\nget.study_and_setting <- function(file){\n  study <<- gsub(file, pattern = \".*(MELR|AVASTM|LUCID|MELR_EXOME|MELR_URINE).*\", replacement = \"\\\\1\")\n  setting <<- gsub(file, pattern = \".*(f0.9_s[0-9]|raw).*\", replacement = \"\\\\1\")\n}\n\nread_snpsift <- function(x){\n  test <- read.table(file = x, fill = T, header = F)\n  #read.table(file = x, quote = F, row.names = F)\n  colnames(test) <- c(\"chr\", \"pos\", \"ref\", \"alt\", \"DP\", \"ADF\", \"ADR\",\"MQSB\", \"file_name\")\n  \n  ## get DP4\n  ref<- filter(test, alt == \".\")\n  test <- filter(test, alt != \".\")\n  \n  # nonref\n  test$ref_F <- as.numeric(unlist(lapply(strsplit(as.character(test$ADF), split = \",\"),\"[[\",1)))\n  test$ref_R <- as.numeric(unlist(lapply(strsplit(as.character(test$ADF), split = \",\"),\"[[\",2)))\n  test$alt_F <- as.numeric(unlist(lapply(strsplit(as.character(test$ADR), split = \",\"),\"[[\",1)))\n  test$alt_R <- as.numeric(unlist(lapply(strsplit(as.character(test$ADR), split = \",\"),\"[[\",2)))\n  \n  test$AF <- (test$alt_F + test$alt_R) /(test$ref_F + test$ref_R + test$alt_F + test$alt_R) \n  \n  # ref\n  ref$ref_F <- as.numeric(as.character(ref$ADF))\n  ref$ref_R <- as.numeric(as.character(ref$ADR))\n  ref$alt_F <- as.numeric(as.character(0))\n  ref$alt_R <- as.numeric(as.character(0))\n  ref$AF <- as.numeric(as.character(0))\n\n  test <- rbind(ref, test)\n  \n  ## Get barcode\n  test$SLX_barcode <- gsub(test$file_name, pattern = \"(SLX-.*_D..._D...).*\", replacement = \"\\\\1\")\n  return(test)\n}\n# Size ----------------------------\nsize_characterisation <- function(files = size_files, round = 1, combine_AVASTM = FALSE){\n  \n  summary <- data.frame()\n  \n  for (x in 1:length(size_files)){\n    curr_study <- gsub(size_files[x], pattern = \".*SIZE.*/(.*s[12345]).*\", replacement = \"\\\\1\")\n    \n    print(paste(\"working on \", curr_study))\n    curr <- readRDS(size_files[x])\n    curr$study <- curr_study\n    \n    summary <- rbind(curr, summary)  \n  }\n  \n  summary <- select(summary, -total, -proportion)\n  \n  ## add size-binned column\n  summary$size.round <- round_any(summary$size, accuracy = round)\n  \n  # identify each study\n  summary$family_size <- gsub(summary$study, pattern = \".*f.*_(s.).*\", replacement = \"\\\\1\")\n  summary$study <- gsub(summary$study, pattern = \".*(MELR|LUCID|AVASTM|MELR_URINE).*\", replacement = \"\\\\1\")\n  summary$study <- factor(summary$study, levels = c(\"LUCID\", \"AVASTM\", \"MELR\", \"MELR_URINE\", \"LUCID/AVASTM\"))\n  \n  if(combine_AVASTM == TRUE){\n    print(\"combining avastm + lucid names\")\n    summary[summary$study == \"LUCID\", 'study'] <- \"LUCID/AVASTM\"\n    summary[summary$study == \"AVASTM\", 'study'] <- \"LUCID/AVASTM\"\n  }\n  \n  print(\"running plyr and rounding\")\n  summary.round <- plyr::ddply(summary, c(\"size.round\", \"study\",\n                                          \"data\", \"case_or_control\",\n                                          \"mut\"), function(x){ data.frame(count = sum(x$count))})\n  \n  summary <- summary.round\n  \n  # fix size output\n  totals <- plyr::ddply(summary, c(\"study\", \"data\", \"case_or_control\", \"mut\"), function(x){\n    data.frame(total = sum(x$count))\n  })\n  \n  summary <- left_join(summary, totals, by = c(\"study\", \"data\", \"case_or_control\", \"mut\"))\n  summary$proportion <- summary$count / summary$total\n  \n  # annotate the data frame\n  summary$sample_type <- grepl(summary$study, pattern = \"URINE\")\n  summary[summary$sample_type == TRUE, \"sample_type\"] <- \"urine_DNA_supernatant\"\n  summary[summary$sample_type == FALSE, \"sample_type\"] <- \"plasma_DNA\"\n  \n  \n  summary$exome <- grepl(summary$study, pattern = \"EXOME\")\n  \n \n  return(summary)\n}\n# Sequencing depth analysis functions  ---------------------------------\nanalysing.raw.sequencing.depth <- function(file, SLX_layout, median_locus_DP_threshold = 100, plot = F){\n  \n  # load file\n  load(file = file, verbose = T)\n  get.study_and_setting(file)\n  print(study)\n  print(setting)\n  \n  # plot DP per locus pre-filtering\n  plot.p29(data_dt.prefilter, median_locus_DP_threshold = 100)\n  \n  data_dt.prefilter$study <- study\n  data_dt.prefilter$setting <- setting\n  \n  SLX_layout <- data.table(SLX_layout)\n  \n  data_dt.prefilter <- dplyr::left_join(data_dt.prefilter,\n                                        SLX_layout[ ,c(\"Study\", \"sample_name\", \"Patient\", \"sample_type\", \"input_into_library_ng\", \"SLX_barcode\", \"case_or_control\")],\n                                        by = \"SLX_barcode\")\n  \n  print(\"Generating summary file per locus\")\n  \n  loci_DP_summmary <- plyr::ddply(data_dt.prefilter, \"uniq_pos\", function(x){\n    median_DP= median(x$DP)\n    mean_DP=mean(x$DP)\n    data.frame(median_DP, mean_DP)\n  })\n  \n  ## Filter data.frame for only loci that had median DP greater than threshold\n  low_depth_loci <- filter(loci_DP_summmary, median_DP < median_locus_DP_threshold)$uniq_pos\n  \n  data_dt.prefilter <- data_dt.prefilter[!(data_dt.prefilter$uniq_pos %in% low_depth_loci), ]\n  \n  median_sample <- median(data_dt.prefilter$DP)\n  mean_sample <- mean(data_dt.prefilter$DP)\n  median_locus <- median(loci_DP_summmary$median_DP)\n  \n  print(paste(\"median per sample =\", median_sample))\n  print(paste(\"mean per sample =\", mean_sample))\n  print(paste(\"median per locus =\", median_locus))\n  \n  if (plot == TRUE){\n  ggplot(data = data_dt.prefilter, aes(x = SLX_barcode, y = DP, fill = case_or_control)) +\n    geom_boxplot() +\n    scale_y_log10() +\n    theme_bw() +\n    labs(x = \"Sample\",\n         y = \"log(10) Depth\",\n         title = paste(\"Sequencing depth\", study, setting),\n         subtitle = paste(\"Median per sample =\", median_sample,\"\\nMedian locus =\", median_locus))+\n    theme(axis.text.x = element_blank())\n  ggsave(paste0(plot_dir, \"p26_\", study, \".\", setting, \".pdf\"), width = 12, height = 6)\n  \n  \n  print(\n    ggplot(data = loci_DP_summmary, aes(x = reorder(uniq_pos, median_DP), y = median_DP)) +\n      geom_point() +\n      scale_y_log10() +\n      theme_classic()+\n      geom_hline(yintercept = median_locus, color = \"hotpink\")+\n      labs(x = \"Locus\",\n           y = \"log(10) Depth\",\n           title = paste(\"Sequencing depth per locus\", study, setting),\n           subtitle = paste(\"Median per sample =\", median_sample,\"\\nMedian locus =\", median_locus, \"\\nLine =\", median_locus))+\n      theme(axis.text.x = element_blank()))\n  ggsave(paste0(plot_dir, \"p27_\", study, \".\", setting, \".pdf\"), width = 9, height = 6)\n  \n  ggplot(data = data_dt.prefilter, aes(x = DP, fill = case_or_control)) +\n    geom_histogram(binwidth = 100)+\n    theme_classic()+\n    geom_vline(xintercept = median_locus, linetype = \"longdash\")+\n    labs(x = \"Locus depth\",\n         y = \"Count\",\n         title = paste(\"Histogram for\", study, setting),\n         subtitle = paste(\"Dashed line: median locus depth =\", median_locus, \"\\nbinwidth = 100\"))+\n    facet_grid(case_or_control~., scales = \"free_y\")\n  ggsave(paste0(plot_dir, \"p28_\", study, \".\", setting, \".pdf\"), width = 7, height = 6)\n  \n  print(\"Done plotting the data\")\n  }\n}\n\nsequencing_depth_per_sample <- function(file, SLX_layout, median_locus_DP_threshold = 100){\n    \n    # load file\n    load(file = file, verbose = T)\n    get.study_and_setting(file)\n    print(study)\n    print(setting)\n    \n    data_dt.prefilter$study <- study\n    data_dt.prefilter$setting <- setting\n    \n    SLX_layout <- data.table(SLX_layout)\n    \n    data_dt.prefilter <- dplyr::left_join(data_dt.prefilter,\n                                          SLX_layout[ ,c(\"Study\", \"sample_name\", \"Patient\", \"sample_type\", \"input_into_library_ng\", \"SLX_barcode\", \"case_or_control\")],\n                                          by = \"SLX_barcode\")\n    \n    print(\"Generating summary file per locus\")\n    \n    loci_DP_summmary <- plyr::ddply(data_dt.prefilter, \"uniq_pos\", function(x){\n      median_DP= median(x$DP)\n      mean_DP=mean(x$DP)\n      data.frame(median_DP, mean_DP)\n    })\n    \n    ## Filter data.frame for only loci that had median DP greater than threshold\n    low_depth_loci <- filter(loci_DP_summmary, median_DP < median_locus_DP_threshold)$uniq_pos\n    \n    data_dt.prefilter <- data_dt.prefilter[!(data_dt.prefilter$uniq_pos %in% low_depth_loci), ]\n    return(data_dt.prefilter)\n}\n# determining ctDNA levels from polished dataframe---------------------------------\nctDNA_level_from_polished.df <- function(path_combined.polished.rds){\n  \n  polished_file <<- readRDS(path_combined.polished.rds)\n  \n  study <- gsub(path_combined.polished.rds, pattern = \".*(MELR|AVASTM|LUCID|MELR_EXOME).*\", replacement = \"\\\\1\")\n  setting <- gsub(path_combined.polished.rds, pattern = \".*(f0.9_s[0-9]).*\", replacement = \"\\\\1\")\n  \n  \n  print(\"filter for lines passing all filters\")\n  polished_file.filtered <- filter(polished_file,\n                                   PASS == TRUE,\n                                   BOTH_STRANDS == TRUE,\n                                   CONTAMINATION_RISK.PASS == TRUE,\n                                   LOCUS_NOISE.PASS == TRUE, \n                                   (data == \"ptspec\")| (data == \"nonptspec\" & COSMIC_MUTATIONS == 0)) %>%\n                                  data.table()\n  \n  print(\"determine AF by sample, mutation, and 3bp context, substract background AF\")\n  ctDNA_level <<- polished_file.filtered[,.(mut_sum = sum(mut_sum),\n                         depth = sum(DP),\n                         sample_AF = sum(mut_sum)/sum(DP),\n                         background_error = unique(background_AF),\n                         background_DP = unique(background.DP),\n                         sample_AF_substracted = (sum(mut_sum)/sum(DP)) - unique(background_AF)), \n                         by = list(data, SLX_barcode, pt_mutation_belongs_to, sample_name, TRINUCLEOTIDE, mut_class)]\n  \n  # ctDNA_level <<- plyr::ddply(polished_file.filtered, c(\"data\", \"SLX_barcode\", \"pt_mutation_belongs_to\", \"sample_name\", \"TRINUCLEOTIDE\", \"mut_class\"), function(x){\n  #   mut_sum <- sum(x$mut_sum)\n  #   depth <- sum(x$DP)\n  #   sample_AF <- mut_sum/depth\n  #   background_error <- unique(x$background_AF)\n  #   background_DP <- unique(x$background.DP)\n  #   sample_AF_substracted <- sample_AF - background_error\n  #   data.frame(mut_sum, depth, sample_AF, background_error, background_DP, sample_AF_substracted)\n  # })\n  \n  print(\"set any AF < 0 to 0\")\n  ctDNA_level$sample_AF_substracted.corrected <- ctDNA_level$sample_AF_substracted\n  ctDNA_level[ctDNA_level$sample_AF_substracted.corrected < 0, ][ , \"sample_AF_substracted.corrected\"] <- 0\n  \n  print(\"determine overall AF per sample, weight by sample depth at given locus\")\n  ctDNA_level_by_sample <- plyr::ddply(ctDNA_level, c(\"data\", \"SLX_barcode\", \"pt_mutation_belongs_to\", \"sample_name\"), function(x){\n    weighted_mean <- weighted.mean(x$sample_AF_substracted.corrected, x$depth)\n    total_mut_sum <- sum(x$mut_sum)\n    total_DP <- sum(x$depth)\n    data.frame(weighted_mean, total_mut_sum, total_DP)\n  })\n  \n  ctDNA_level_by_sample$weighted_mean.plotting <- ctDNA_level_by_sample$weighted_mean + 1e-08\n  \n  print(ggplot(data = ctDNA_level_by_sample, aes(x = reorder(sample_name, weighted_mean), y = weighted_mean, color = data))+\n          geom_point()+\n          scale_y_log10()+\n          theme_bw()+\n          theme(axis.text.x = element_blank())+\n          labs(x = \"Sample\",\n               y = \"Weighted AF\",\n               title = \"ctDNA levels in ptspec and nonptspec\",\n               subtitle = paste(study, setting)))\n  \n  colnames(ctDNA_level_by_sample) <- c(\"data\", \"SLX_barcode\", \"pt_mut_belongs_to\", \"sample_name\", \"weighted_mean\", \"mut_sum\", \"total_DP\", \"weighted_mean.plotting\")\n  ctDNA_level_by_sample$pt_mut_belongs_to <- as.character(ctDNA_level_by_sample$pt_mut_belongs_to)\n  return(ctDNA_level_by_sample)\n}\n\nctDNA_level_from_polished.df.class <- function(path_combined.polished.rds){\n  \n  polished_file <<- readRDS(path_combined.polished.rds)\n  \n  study <- gsub(path_combined.polished.rds, pattern = \".*(MELR|AVASTM|LUCID|MELR_EXOME).*\", replacement = \"\\\\1\")\n  setting <- gsub(path_combined.polished.rds, pattern = \".*(f0.9_s[0-9]).*\", replacement = \"\\\\1\")\n  \n  \n  print(\"filter for lines passing all filters\")\n  polished_file.filtered <- filter(polished_file,\n                                   PASS == TRUE,\n                                   BOTH_STRANDS == TRUE,\n                                   CONTAMINATION_RISK.PASS == TRUE,\n                                   LOCUS_NOISE.PASS == TRUE, \n                                   (data == \"ptspec\")| (data == \"nonptspec\" & COSMIC_MUTATIONS == 0)) %>%\n    data.table()\n  \n  # get error rate by class - need to polish across trinucleotide contexts\n  background_rates <- polished_file.filtered[,c(\"mut_class\", \"TRINUCLEOTIDE\", \"background.mut_sum\", \"background.DP\", \"background_AF\")]\n  background_rates$class <- paste0(background_rates$mut_class, \"_\", background_rates$TRINUCLEOTIDE)\n  background_rates <- background_rates[!duplicated(background_rates$class),]\n  \n  background_rates.by_class <- plyr::ddply(background_rates, \"mut_class\", function(x){\n    data.frame(background_AF = (sum(as.numeric(x$background.mut_sum)) / sum(as.numeric(x$background.DP))))\n  })\n  \n  print(\"determine AF by sample, mutation, and 3bp context, substract background AF\")\n  ctDNA_level <<- polished_file.filtered[,.(mut_sum = sum(mut_sum),\n                                            depth = sum(DP),\n                                            sample_AF = sum(mut_sum)/sum(DP)),\n                                            by = list(data, SLX_barcode, pt_mutation_belongs_to, sample_name, mut_class)]\n  \n  ctDNA_level <- left_join(ctDNA_level, background_rates.by_class, by = \"mut_class\")\n  ctDNA_level$corrected_AF <- ctDNA_level$sample_AF - ctDNA_level$background_AF\n  ctDNA_level$corrected_AF <- pmax(ctDNA_level$corrected_AF, 0)\n  \n  print(\"determine overall AF per sample, weight by sample depth at given locus\")\n  ctDNA_level_by_sample.by_class <- plyr::ddply(ctDNA_level, c(\"data\", \"SLX_barcode\", \"pt_mutation_belongs_to\", \"sample_name\"), function(x){\n    weighted_mean <- weighted.mean(x$corrected_AF, x$depth)\n    total_mut_sum <- sum(x$mut_sum)\n    total_DP <- sum(x$depth)\n    data.frame(weighted_mean, total_mut_sum, total_DP)\n  })\n  \n  ctDNA_level_by_sample.by_class$weighted_mean.plotting <- ctDNA_level_by_sample.by_class$weighted_mean + 1e-08\n  \n  return(ctDNA_level_by_sample.by_class)\n}\n# Mixture experiment----------------------------\nparse_mixture_INVAR_scores <- function(mixture){\n  mixture$overall_filter <- gsub(mixture$overall_filter, pattern = \".*(using_size|no_size).*\", replacement = \"\\\\1\")\n  starting_AF <- median(filter(mixture, grepl(\"1x\", Timepoint))$adjusted_ctDNA)\n  mixture$dilution <- as.numeric(gsub(mixture$Timepoint, pattern = \"x\", replacement = \"\"))\n  mixture$expected_AF <- starting_AF / mixture$dilution\n  \n  return(mixture)\n}\n\n\n# Discussion about levels of ctDNA\ndetected_proportion_by_threshold <- function(AVASTM_detection.3M, study = c(\"AVASTM\", \"LUCID\"), setting = \"f0.9_s2\"){\n  output <- data.frame()\n  \n  #filter(AVASTM_detection.3M, DP )\n  \n  print(\"running loop\")\n  for(x in c(c(5 %o% 10^(-3:-5)), c(1 %o% 10^(-2:-5)))){\n    proportion_detected <- nrow(AVASTM_detection.3M[AVASTM_detection.3M$adjusted_ctDNA > x & AVASTM_detection.3M$detection == TRUE,])/\n                                  nrow(AVASTM_detection.3M)\n    output <- rbind(data.frame(threshold = x, proportion_detected = proportion_detected), output)\n  }\n  \n  print(\"lm\")\n  ctDNA_model.AVASTM <- lm(proportion_detected ~ log10(threshold), data = output)\n  print(summary(ctDNA_model.AVASTM))\n  output.extrapolated <- extrapolate_threshold(ctDNA_model.AVASTM)\n  \n  output.extrapolated$data <- \"prediction\"\n  output$data <- \"actual\"\n  \n  colnames(output.extrapolated) <- colnames(output)\n  combined_detection_rates <<- rbind(output, output.extrapolated)\n  \n  rsquared <<- summary(ctDNA_model.AVASTM )$r.squared\n  p_value <<- summary(ctDNA_model.AVASTM )$coefficients[2,4]\n  \n  print(\"plotting\")\n  print(ggplot(combined_detection_rates, aes(x = log10(threshold), y = proportion_detected, colour = data))+\n          geom_point()+\n          geom_smooth(method = \"lm\")+\n          #scale_x_log10()+\n          scale_x_reverse(limits = c(-2,-9))+\n          labs(x = \"log10 ctDNA detection threshold\",\n               y = \"Proportion of samples detected\",\n               title = \"Proportion of samples detected with varying threshold\",\n               subtitle = paste(study, \"r2 = \",signif(rsquared, 2), \",p = \", signif(p_value, 2)))+\n          theme_bw()+\n          scale_y_continuous(limits = c(0,1.1)))\n  \n  ggsave(paste0(plot_dir, \"p66_\", study, \"_\", setting, \".detection_rates_by_threshold.pdf\"), width = 6, height = 5)\n  \n  return(combined_detection_rates )\n}\nextrapolate_threshold <- function(ctDNA_model.AVASTM){\n  extrapolate_output <- data.frame()\n  \n  for(extrapolate_threshold in c(1e-6,1e-7,1e-8,1e-9, 1e-10)){\n    prediction <- predict(ctDNA_model.AVASTM, newdata = data.frame(threshold = extrapolate_threshold))\n    \n    extrapolate_output <- rbind(data.frame(extrapolate_threshold = extrapolate_threshold, prediction = prediction), extrapolate_output)\n  }\n  return(extrapolate_output)\n}\n# TAm-Seq functions --------------------------\n## Functions that interrogate the clinical datasheet given by the CCTC\nparse_clinical_data <- function(path_to_clin_csv){\n  patient_data <- read.csv(path_to_clin_csv, header=TRUE, fill=TRUE) #update as MelR data is updated \n  filter(patient_data, Site == \"n01\") -> patient_data\n  dplyr::select(patient_data, Label, BASEASSESSDAT, TUMASS2DAT,OVERALLRES, TRTDRGNAME, STDAT, ENDAT) -> patient_data_filtered\n  \n  filter(patient_data_filtered, patient_data_filtered$BASEASSESSDAT != \"\" | patient_data_filtered$TUMASS2DAT != \"\"\n         | patient_data_filtered$OVERALLRES != \"\"| patient_data_filtered$TRTDRGNAME != \"\"\n         | patient_data_filtered$STDAT != \"\" | patient_data_filtered$ENDAT != \"\") -> a\n  \n  # REGEX to tidy up aa_positions table\n  pattern <- \"(.....).+\"\n  str_match(a$Label, pattern) -> patient.IDs\n  patient.IDs[,2] -> a$Label\n  \n  # Replace overall response codes into words \n  \n  a$OVERALLRES[a$OVERALLRES == 1] <- \"CR\"\n  a$OVERALLRES[a$OVERALLRES == 2] <- \"PD\"\n  a$OVERALLRES[a$OVERALLRES == 3] <- \"PR\"\n  a$OVERALLRES[a$OVERALLRES == 4] <- \"SD\"\n  a$OVERALLRES[a$OVERALLRES == 5] <- \"Inevaluable\"\n  \n  as.Date(a$BASEASSESSDAT, \"%d/%m/%Y\") -> baseline_dates\n  baseline_dates[!is.na(baseline_dates)] -> baseline_dates1\n  \n  sort(unique(a$Label)) -> patient_name_list ##necessary for new IDs as they are no longer integers\n  \n  clin_list <- rep(0,length(unique(a$Label))) # initialise a list so that the names of each of the separate sheets can be listed\n  reltve_list <- rep(0,length(unique(a$Label)))\n  \n  i = 1 # split big table into multiple data frames\n  for (i in 1:length(unique(a$Label))){\n    as.data.frame(filter(a, Label == patient_name_list[i])) -> j\n    assign(paste(\"clin\", patient_name_list[i], sep = \"_\"), j)\n    paste(\"clin\", patient_name_list[i], sep = \"_\") -> clin_list[i]\n  }\n  \n  z=1 #Change each of the dataframes dates to numbers since baseline assessment\n  for (z in 1:length(patient_name_list)){\n    get(clin_list[z]) -> y\n    \n    as.numeric(as.Date(y[,3], \"%d/%m/%Y\") - as.Date(y[1,2], \"%d/%m/%Y\")) -> y[,3]\n    as.numeric(as.Date(y[,6], \"%d/%m/%Y\") - as.Date(y[1,2], \"%d/%m/%Y\")) -> y[,6]\n    as.numeric(as.Date(y[,7], \"%d/%m/%Y\") - as.Date(y[1,2], \"%d/%m/%Y\")) -> y[,7]\n    \n    y$STDAT[y$STDAT < 0] <- 0 #get rid of any negative numbers\n    y$OVERALLRES[y$OVERALLRES < 0] <- 0 \n    assign(paste(\"clin\", patient_name_list[z], sep = \"_\"), y)\n    \n  }\n  \n  patients <- ls(pattern = 'clin_')[c(2,4,6,10,12,14,21,23,34)]\n  assign(x= 'patients_altnames', value = substr(ls(pattern = 'clin_')[c(2,4,6,10,12,14,21,23,34)], 6, 10), envir = globalenv())\n  clin <- do.call(rbind, mget(patients))\n  \n  clin$Label <- paste0(\"MR1\", substr(as.character(clin$Label), 1, 3))\n  assign(x = 'patients', unique(clin$Label), envir = globalenv())\n  row.names(clin) <- NULL\n  return(clin)\n}\nget_LDH <- function(path_to_clin_csv){\n  patient_data <- read.csv(path_to_clin_csv, header=TRUE, fill=TRUE) #update as MelR data is updated \n  filter(patient_data, Site == \"n01\") -> patient_data\n  \n  LDH <- dplyr::select(patient_data, Label, LDHMEAS, BASEASSESSDAT, LDHDAT, LDHMEAS1)  \n  LDH <- LDH[substr(LDH$Label, 1, 5) %in% patients_altnames,]\n  LDH_1 <- LDH[,c(1,3,2)]\n  LDH_2 <- LDH[c(1,4,5)]\n  \n  colnames(LDH_1) <- colnames(LDH_2)\n  LDH <- rbind(LDH_1, LDH_2)\n  LDH <- LDH[!is.na(LDH$LDHMEAS1),]\n  \n  ## convert names to new style\n  LDH$Label <- paste0(\"MR1\", substr(as.character(LDH$Label), 1, 3))\n  \n  assign(x='LDH', value=LDH, envir = globalenv())\n} \n\n## Functions that interrogate the biorepository spreadsheet from 'groups/Research/nrlab/group_folders/PROJECTS/CLINICAL\\ STUDIES/MELRESIST\\ \\(MELR\\)/CRI\\ BIOREPOSITORY\\ SAMPLES/CURRENT\\ VERSION'\nget_blood_test_dates <- function(path_to_biorep_csv){\n  blood.test.dates <- read.csv(path_to_biorep_csv, header=TRUE, fill=TRUE) ##update as sample list is updated\n  blood.test.dates[-c(1:6),] -> blood.test.dates #deletes the first 6 rows by selecting the complement to keep\n  pattern <- \"0(.).(0..)\" \n  str_match(blood.test.dates[,2], pattern) ->  test\n  paste(test[,3], test[,2], sep=\"_\") -> test1\n  test1 -> blood.test.dates[,2]\n  \n  filter(blood.test.dates, X.3 == \"PLASMA\") -> blood.test.dates\n  dplyr::select(blood.test.dates, X.1, X.5) -> blood.test.dates\n  \n  blood.test.dates[,1] <- paste0(\"MR1\", substr(as.character(blood.test.dates[,1]), 1, 3))\n  \n  assign(value = clin[!(clin$BASEASSESSDAT == \"\"),1:2], x = 'baseline_dates', envir = globalenv())\n  \n  for (z in 1:length(patients)){\n    dates_temp <- filter(blood.test.dates, X.1 == patients[z]) \n    \n    dates_temp$days <- as.numeric(as.Date(dates_temp[,2], \"%d/%m/%Y\") - as.Date(baseline_dates[z,2], \"%d/%m/%Y\")) \n    dates_temp$days[dates_temp$days < 0] <- 0 #remove any negatives\n    blood.test.dates[blood.test.dates$X.1 == patients[z],3] <- dates_temp$days\n  }\n  \n  blood.test.dates <- blood.test.dates[!is.na(blood.test.dates$V3),] \n  blood.test.dates <- blood.test.dates[order(blood.test.dates$X.1, blood.test.dates$V3),]\n  blood.test.dates$uniq <- paste(blood.test.dates$X.1, blood.test.dates$X.5, sep=\"_\")\n  blood.test.dates <- blood.test.dates[!duplicated(blood.test.dates$uniq),1:3]\n  \n  colnames(blood.test.dates) <- c('patient', 'date_of_sample', 'days')\n  \n  ## add timepoint info to blood.test.dates\n  library(plyr)\n  blood.test.dates <- ddply(blood.test.dates, .(patient), mutate, id=seq_along(days))\n  colnames(blood.test.dates)[4] <- 'timepoint'\n  \n  assign(blood.test.dates, x='blood.test.dates', envir = globalenv())\n  print(\"Generated blood.test.dates object\")\n} # Get blood test dates from biorepository sample list \n\n##--------------- Functions that extract data from raw TAm-Seq .txt files\nimport_raw_data_test_samples <- function(){\n  print(paste('current dir is:', getwd()))\n  files_to_read <- list.files(\".\", recursive = T, pattern = \"afs.txt|reads.txt|depths.txt\")\n  print(paste('Importing:',files_to_read))\n  \n  raw_test <- data.frame()\n  \n  raw_list <- list() \n  \n  for (x in 1:length(files_to_read)){\n    raw <- read.table(files_to_read[x], header = T)\n    raw$uniq <- paste(paste0('chr',raw$Chromosome), raw$Position, raw$Reference, raw$Mutation, raw$Gene, sep=\"_\")\n    raw$name  <- files_to_read[x]\n    \n    raw_list[[x]] <- melt(data.frame(raw$name, raw$uniq, dplyr::select(raw, contains(\"FLD\"))))\n    print(paste('done', files_to_read[x]))\n  }\n  \n  ## make a dataframe\n  raw_test <- do.call(rbind, raw_list)\n  \n  ### split up raw_test into afs, reads, depths\n  SLX_pt_type <- strsplit(as.character(raw_test$raw.name), split=\"/\")\n  \n  raw_test$SLX <- lapply(SLX_pt_type, '[[', 1)\n  \n  pt_type <- lapply(SLX_pt_type, '[[', 2)\n  \n  raw_test$patient <- substr(pt_type, 1,6)\n  raw_test$type <- substr(pt_type, 8,nchar(pt_type)-4)\n  raw_test <- raw_test[,c(5:7,2:4)]\n  \n  raw_test$uniq2 <- paste(raw_test$SLX, raw_test$patient, raw_test$raw.uniq, raw_test$variable, raw_test$type,  sep=\"_\")\n  raw_test$uniq3 <- paste(raw_test$SLX, raw_test$patient, raw_test$raw.uniq, raw_test$variable, sep=\"_\")\n  raw_test <- raw_test[!duplicated(raw_test$uniq2),-7]\n  \n  afs<- filter(raw_test, type=='afs')\n  reads<- filter(raw_test, type=='reads')\n  depths <- filter(raw_test, type=='depths')\n  \n  merge1 <- merge(afs, reads, by='uniq3')\n  merge2 <- merge(merge1, depths, by='uniq3')\n  \n  merged <- merge2[,c(2,3,5,6,7,13,19,1)]\n  colnames(merged) <- c('SLX', 'patient', 'mut_uniq','FLD','AF','reads','depth','uniq')\n  return(merged[,1:7])\n} \n## Swap barcodes for real names\nswap_barcodes <- function(x){\n  x$key <- paste(x$SLX, x$FLD, sep=\"_\") \n  barcodes$key <- paste(barcodes$SLX_ID, barcodes$Barcode, sep=\"_\")\n  x <- merge(x, barcodes, by = 'key')\n  return(x[,c(2:8, 12:16)])\n}\n# Plots ---------------------\nplot.p29 <- function(data_dt.prefilter, median_locus_DP_threshold = 100){\n  samples_count <- data_dt.prefilter[,.(total_samples=length(unique(SLX_barcode))), \n                                     by = uniq_pos]\n  \n  median_DP <- data_dt.prefilter[,.(median=median(DP)), \n                                 by = uniq_pos]\n  \n  per_sample <- inner_join(median_DP, samples_count, by = \"uniq_pos\")\n  per_sample$proportion_of_samples <- per_sample$total_samples / length(unique(data_dt.prefilter$SLX_barcode))\n  \n  print(\n    ggplot(per_sample, aes(x = proportion_of_samples, y = median))+\n      geom_jitter(alpha = 0.6)+\n      scale_y_log10(breaks = c(10,100,1000,10000))+\n      geom_hline(yintercept = median_locus_DP_threshold) +\n      theme_bw() +\n      labs(x = \"Proportion of samples with sequencing coverage\",\n           y = \"Median log(10) Depth per locus\",\n           title = paste(\"Characterisation of sequencing depth per locus\", study, setting),\n           subtitle = paste(\"Median depth threshold =\", median_locus_DP_threshold)))\n  ggsave(paste0(plot_dir, \"p29_\", study, \".\", setting, \".pdf\"), width = 8, height = 5)\n}\n\n\n\np37 <- function(file, save = F){\n  \n  size_prob <- readRDS(file)\n  size_prob <- rbind(size_prob[[1]], size_prob[[2]])\n  \n  smooth <- gsub(file, pattern = \".*smooth_(0\\\\.[0-9]+)\\\\..*\", replacement = \"\\\\1\", perl = T)\n  study <- gsub(file, pattern = \".*(LUCID|MELR|MELR_EXOME|AVAST_M).*\", replacement = \"\\\\1\", perl = T)\n  \n  print(ggplot(size_prob, aes(x = fragment_length, y = probability, fill = mut)) +\n          geom_bar(stat = \"identity\", position = \"dodge\") +\n          labs(x = \"Fragment size in bp\",\n               y = \"Proportion\",\n               title = \"Size profile following smoothing\",\n               subtitle = paste(study, \"data; smooth =\", smooth)) +\n          theme_classic() +\n          theme(axis.text = element_text(size = 12),\n                axis.title = element_text(size = 14,\n                                          face = \"bold\"),\n                legend.text = element_text(size = 12),\n                legend.title = element_text(size = 12,\n                                            face = \"bold\")) +\n          scale_x_continuous(limits = c(0, 400)))\n  \n  # is total prob is equal to 1?\n  print(\n    plyr::ddply(size_prob, \"mut\", function(x){\n      sum(x$probability)\n    }))\n  \n  if (save == T){\n    print(\"saving\")\n    ggsave(paste0(plot_dir, \"p37_\", study, \".\", \"smooth_\", smooth, \".size_profile_following_smoothing.pdf\"), width = 6, height = 4)\n  }\n}\n\np50 <- function(error_file, SLX_layout, exclude_PPC = T, save = T){\n  print(\"note that this downsampling step takes 5-10 mins to run.\")\n  # load background rates\n  background_error <- calculate.background_error(error_file, SLX_layout, exclude_PPC = T)\n  background_error <- add.missing_error_classes(background_error, trinucleotide_depth)\n  background_error <- combine_classes(background_error)\n  \n  # set levels\n  background_error$data <- factor(background_error$data, levels = c(\"one_read\", \"both_reads\", \"locus_noise\", \"locus_noise.both_reads\"))\n  \n  # Need to downsample case data to the size of the controls\n  background_error$key <- paste0(background_error$TRINUCLEOTIDE, \"_\", background_error$mut_class)\n  \n  print(\"downsampling now, please wait.\")\n  test <- background_error %>%\n    plyr::ddply(c(\"data\", \"key\"), function(x){\n      print(unique(x$key))\n      \n      mut <- filter(x, case_or_control == \"case\")$background.mut_sum\n      DP <- filter(x, case_or_control == \"case\")$background.DP\n      vector <- c(rep(1, mut), rep(0, DP-mut))\n      \n      new.mut <- sum(sample(vector, filter(x, case_or_control == \"control_negative\")$background.DP))\n      new.DP <- filter(x, case_or_control == \"control_negative\")$background.DP\n      \n      \n      data.frame(new.mut, new.DP, new.background_AF = new.mut/new.DP)\n    })\n  \n  original <- background_error %>%\n    plyr::ddply(c(\"data\", \"key\"), function(x){\n      print(unique(x$key))\n      \n      mut <- filter(x, case_or_control == \"control_negative\")$background.mut_sum\n      DP <- filter(x, case_or_control == \"control_negative\")$background.DP\n      \n      data.frame(new.mut = mut, new.DP = DP, new.background_AF = mut/DP)\n    })\n  \n  test$case_or_control <- \"case\"\n  original$case_or_control <- \"control_negative\"\n  \n  background_error.corrected <- rbind(test, original)\n  background_error.corrected$mut_class <- unlist(lapply(strsplit(background_error.corrected$key, split = \"_\"), \"[[\", 2))\n  colnames(background_error.corrected)[which(grepl(colnames(background_error.corrected), pattern = \"background_AF\"))] <- \"background_AF\"\n  \n  # ggplot(background_error.corrected, aes(x = mut_class, y = background_AF, fill = case_or_control)) +\n  #   geom_boxplot() +\n  #   scale_y_log10(breaks = c(1e-7, 1e-6, 1e-5, 1e-4, 1e-3))+\n  #   theme_classic() +\n  #   scale_fill_manual(breaks = c(\"case\", \"control_negative\"),\n  #                     labels = c(\"Patient\", \"healthy control\"),\n  #                     name = \"\",\n  #                     values = c(\"mediumorchid3\", \"steelblue2\")) +\n  #   labs(x= \"Mutation class\",\n  #        y= \"Background AF\",\n  #        title= paste(\"Background error comparison in 10bp window\"),\n  #        subtitle = paste(study, setting))+\n  #   facet_grid(data ~ .)\n  # ggsave(paste0(plot_dir, \"p8_\", study, \"_\", setting, \".pdf\"), width = 7, height = 6)\n  \n  ## compare error rates between case and control\n  case <- filter(background_error.corrected, case_or_control == \"case\")\n  control <- filter(background_error.corrected, case_or_control != \"case\")\n  \n  comparison_between_case_control <- left_join(case, control[,c(1,2,5)], by = c(\"data\", \"key\"))\n  \n  ggplot(comparison_between_case_control, aes(x = background_AF.x, y = background_AF.y, colour = mut_class))+\n    geom_point()+\n    #geom_smooth(method = \"lm\")+\n    scale_y_log10()+\n    scale_x_log10()+\n    facet_grid(data ~ .)+\n    geom_abline(slope = 1, linetype = \"dashed\")+\n    labs(x = \"Background AF - patients as controls\",\n         y = \"Background AF - healthy controls\",\n         title = \"Comparison of background error rates in 10bp window\",\n         subtitle = paste(study, setting)) +\n    theme_classic()+\n    guides(fill=FALSE) +\n    theme(axis.text=element_text(size=12),\n          axis.title=element_text(size=14,face=\"bold\"),\n          panel.grid.major = element_line(colour = alpha(\"black\", 0.1)))\n  \n  if (save == TRUE){\n    ggsave(paste0(plot_dir, \"p50_\", study, \"_\", setting, \".background_AF_true_vs_false_control.pdf\"), width = 6, height = 7)\n  }\n  \n  print(\n    plyr::ddply(comparison_between_case_control, \"data\", function(x){\n      print(unique(x$data))\n      print(data.frame(r.squared = summary(lm(background_AF.x ~ background_AF.y, data = x))$r.squared))\n    }))\n  \n  return(comparison_between_case_control)\n}\n\nplot_effects_of_different_smooths <- function(file, smooth){\n  size_characterisation <- readRDS(file)\n  \n  ## select columns depending on what data has been used\n  if (exists(\"size.combined\") == TRUE){\n    print(\"Warning: you are using an old version of the size_characterisation data\")\n    size_characterisation <- size.combined\n  } else {\n    size_characterisation <- filter(size_characterisation, data == \"ptspec\") %>%\n      select(mut, total, size, count, proportion)\n  }\n  colnames(size_characterisation)[3] <- \"length.round\"\n  ggplot2::ggplot(size_characterisation, aes(x = length.round, y = proportion, colour = mut))+\n    geom_point()\n  \n  min_length <- 0#min(filter(size_characterisation, mut == TRUE)$length.round)\n  max_length <- 400#max(filter(size_characterisation, mut == TRUE)$length.round)\n  \n  ## from detection_functions \n  source(\"~/tapas_pipeline/R/detection_functions.R\")\n  size.combined <- size_characterisation\n  \n  ## read length probabilites for mutant reads\n  output <- data.frame()\n  \n  for (boolean in c(TRUE, FALSE)){\n    print(\"generating read length probabilities\")\n    print(boolean)\n    fragment_length <- size.combined$length.round[size.combined$mut == boolean]\n    counts <- size.combined$count[size.combined$mut == boolean]\n    \n    for (smooth in seq(0.03, 1, by = 0.1)){\n      print(smooth)\n      bw_adjust = smooth\n      \n      probs_mut <- estimate_real_length_probability(fragment_length, \n                                                    counts, \n                                                    min_length = min_length, \n                                                    max_length = max_length,\n                                                    bw_adjust = as.numeric(smooth))\n      probs_mut$smooth <- smooth\n      probs_mut$mutant <- boolean\n      \n      output <- rbind(probs_mut, output)\n    }\n    \n  }\n  \n  output$smooth <- factor(output$smooth)\n  return(output)\n}\n\nbackground_polish.pool <- function(combined, cluster_or_local = c(\"cluster\", \"local\"), p_value = 0.05){\n  combined <- data.table(combined)\n  \n  if(\"prob\" %in% colnames(combined)){\n    print(\"re-polishing\")\n    combined <- select(combined, -average, -p_threshold, -prob, -PASS)\n  }\n  \n  combined.pooled <- combined[LOCUS_NOISE.PASS == TRUE & BOTH_STRANDS == TRUE,\n           .(mut_sum = sum(ALT_F)+sum(ALT_R),\n             DP = sum(DP)),\n           by=list(uniq_pos, Patient, pt_mutation_belongs_to, data)]\n  combined.pooled$AF <- combined.pooled$mut_sum /combined.pooled$DP\n\n  \n  ## calculate a p-value threshold for each sample based on the average AF\n  mean_sd <- combined.pooled[AF < 0.1 ,\n                       .(average = weighted.mean(AF, DP),\n                         p_threshold = p_value/length(AF)),\n                       by=list(Patient, pt_mutation_belongs_to, data)]\n    \n    combined.test <<- inner_join(combined.pooled, mean_sd[,c(\"Patient\", \"pt_mutation_belongs_to\", \"average\", \"p_threshold\", \"data\")], by = c(\"Patient\", \"pt_mutation_belongs_to\", \"data\"))\n  \n  #apply binomial test\n  combined.test$prob <- mapply(binom,\n                               x = combined.test$mut_sum,\n                               n = combined.test$DP,\n                               p = combined.test$average)\n  \n  print(\"annotating with PASS polish or not\")\n  combined.test$PASS <- combined.test$prob > combined.test$p_threshold\n  \n  ## proportion of data removed by outlier suppression\n  filter(combined.test, AF> 0, AF < 0.1) %>%\n    plyr::ddply(  \"data\", function(x){\n      nrow_pass <- nrow(filter(x, PASS == TRUE))\n      #nrow_fail <-  nrow(filter(x, PASS != TRUE))\n      data.frame(proportion = nrow_pass/nrow(x), total_rows = nrow(x))\n    })\n  \n  #p34( p_value.threshold = 0.05, study, setting, save = F)\n  combined.test$sample_name <- paste0(combined.test$Patient, \"_\", combined.test$pt_mutation_belongs_to)\n    \n  filter(combined.test, AF > 0, AF< 0.1) %>%\n    ggplot2::ggplot(aes(x = sample_name, y = AF, colour = PASS ))+\n    geom_point()+\n    theme_classic()+\n    scale_y_log10()+\n    facet_grid(data ~ .)+\n    xlab(\"Sample\") +\n    ylab(\"Mutant allele fraction\")+\n    theme(axis.text.x=element_blank())+\n    labs(title = paste0(\"Effect of outlier-suppression\"))\n  \n  ptspec.pass.pooled <- filter(combined.test, PASS == TRUE, data == \"ptspec\", AF> 0)\n  nonptspec.pass.pooled <- filter(combined.test, PASS == TRUE, data == \"nonptspec\", AF> 0)\n  \n  return(combined.test)\n}\n\n# ctDNA level plot\nplotting_ctDNA_levels <- function(ctDNA_DF, QC_DP_threshold = 20000){\n  # rename colnames\n  colnames(ctDNA_DF)[grepl(colnames(ctDNA_DF), pattern = \"DP\")] <- \"total_DP\"\n  \n  print(\"filtering for overall filter INVAR_SCORE.using_size_TRUE_TRUE_TRUE and ptspec data\")\n  ctDNA_DF_filtered <- filter(ctDNA_DF, overall_filter == \"INVAR_SCORE.using_size_TRUE_TRUE_TRUE\", data == \"ptspec\")\n  print(\"Generating color sheme for plot\")\n  ctDNA_DF_filtered$coloring <- as.character(ctDNA_DF_filtered$detection)\n  ctDNA_DF_filtered$sufficient_DP <- ctDNA_DF_filtered$total_DP > QC_DP_threshold\n  ctDNA_DF_filtered[ctDNA_DF_filtered$total_DP < QC_DP_threshold & ctDNA_DF_filtered$coloring == FALSE, \"coloring\"] <- \"insufficient_DP\"\n\n  print(\"Plotting and saving data\")\n  print(ggplot(data = ctDNA_DF_filtered, aes(x = reorder(pt_mut_belongs_to, adjusted_ctDNA), y = adjusted_ctDNA))+\n          geom_point(aes(color = coloring), size = 3) +\n          scale_y_log10(limits = c(1e-6,1))+\n          labs(x = \"sample\",\n               y = \"log(10) ctDNA level INVAR\",\n             title = \"Patient ctDNA level by sample\",\n             subtitle = paste(study, setting, \"\\nOnly showing filter: INVAR_SCORE.using_size_TRUE_TRUE_TRUE\\nQC filter = \", QC_DP_threshold))+\n          theme_bw()+\n          theme(axis.text.x = element_text(angle = 45,  hjust = 1))+\n          scale_color_discrete(name = \"Detection\",\n                               breaks = c(\"TRUE\", \"FALSE\", \"insufficient_DP\"),\n                              labels = c(\"Detected\", \"Not detected\", \"QC fail\")))\n  ggsave(paste0(plot_dir, \"p43_\", study, \"_\", setting, \".pdf\"), width = 9, height = 7)\n}\n\nplot.kaplan_meier <- function(lead_time.km){\n  library(GGally)\n  ggsurv(lead_time.km, size.est = 1.1, plot.cens = F)+\n    theme_bw()+\n    guides(colour=guide_legend(\"ctDNA at nadir < threshold\"))+\n    labs(y= 'Proportion surviving')+\n    labs(x= 'Time from nadir')+\n    theme(axis.text=element_text(size=12),\n          axis.title=element_text(size=14,face=\"bold\"),\n          legend.text=element_text(size=12),\n          legend.title=element_text(size=12,face=\"bold\"))+\n    coord_cartesian(xlim = c(0,400))+\n    labs(title = \"Lowest ctDNA vs. survival time\")\n}\n\n\n",
    "created" : 1524560070891.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "0|8|222|1|\n1|89|97|0|\n99|35|127|0|\n13|64|15|2|\n129|37|146|0|\n147|40|157|0|\n160|76|186|0|\n188|40|191|0|\n193|28|222|0|\n223|7|279|1|\n1|89|56|0|\n38|91|40|2|\n280|39|393|1|\n1|104|81|0|\n22|77|26|2|\n83|91|113|0|\n19|79|23|4|\n394|50|507|1|\n1|69|64|0|\n41|131|46|2|\n66|75|113|0|\n22|86|24|2|\n37|140|42|2|\n508|20|572|1|\n1|48|8|0|\n12|117|54|0|\n55|54|64|0|\n635|38|652|0|\n655|53|692|0|\n695|43|741|0|\n749|8|1060|1|\n1|73|22|0|\n26|32|59|0|\n25|46|27|4|\n61|67|156|0|\n15|46|27|4|\n30|46|37|4|\n89|69|92|4|\n158|60|208|0|\n210|103|268|0|\n34|38|38|4|\n271|69|296|0|\n298|44|311|0|\n",
    "hash" : "269623538",
    "id" : "4921AE23",
    "lastKnownWriteTime" : 1524560171,
    "last_content_update" : 1524573099954,
    "path" : "~/tapas_paper/functions.R",
    "project_path" : null,
    "properties" : {
        "docOutlineVisible" : "1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}